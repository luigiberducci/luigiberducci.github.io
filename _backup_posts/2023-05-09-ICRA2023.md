---
layout: post
title: Accepted Paper at ICRA2023 Rap4Robots Workshop
subtitle: Our work on learning human-attention feature maps for robot learning has been accepted
cover-img: /assets/img/RAP4Robots-exp-setup.png
thumbnail-img: /assets/img/RAP4Robots-exp-setup.png
share-img: /assets/img/RAP4Robots-exp-setup.png
tags: [human-attention, real-world, f1tenth, imitation learning, object detection]
---

On behalf of the co-authors, I am proud to announce that our paper:

**Enhancing Robot Learning through Learned Human-Attention Feature Maps**
*Daniel Scheuchenstuhl\*, Stefan Ulmer\*, Felix Resch\*, Luigi Berducci, Radu Grosu.*

has been accepted at the 
*Workshop on effective Representations, Abstractions, and Priors for Robot Learning (RAP4Robots)*,
held in the International Conference on Robotics and Automation (**ICRA 2023**).

**Abstract**:
Robust and efficient learning remains a challenging
problem in robotics, in particular with complex visual inputs.
Inspired by human attention mechanism, with which we quickly
process complex visual scenes and react to changes in the
environment, we think that embedding auxiliary information
about focus point into robot learning would enhance efficiency
and robustness of the learning process. In this paper, we propose
a novel approach to model and emulate the human attention with
an approximate prediction model. We then leverage this output
and feed it as a structured auxiliary feature map into downstream
learning tasks. We validate this idea by learning a prediction
model from human-gaze recordings of manual driving in the
real world. We test our approach on two learning tasks - object
detection and imitation learning. Our experiments demonstrate
that the inclusion of predicted human attention leads to improved
robustness of the trained models to out-of-distribution samples
and faster learning in low-data regime settings. Our work
highlights the potential of incorporating structured auxiliary
information in representation learning for robotics and opens
up new avenues for research in this direction.


[Paper](https://arxiv.org/abs/2308.15327v1)

[Code](https://github.com/CPS-TUWien/learning_human_attention)

[Video - Coming soon]()

